{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocessing on spatial data\n",
      "File ../data/cr_tutorialA_cropped.npy already exists and has been loaded instead.\n",
      "No preprocessing on trace data.                   ../data/cr_tutorialCraw_normalized.npy already                   exists and has been loaded instead.\n",
      "Successfully loaded data.\n",
      "Training and test data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11603, 6900)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cnmfereview as cr\n",
    "import config as cfg\n",
    "import os\n",
    "from joblib import dump, load\n",
    "\n",
    "MODELDIR = Path('../best_models')\n",
    "\n",
    "data = cr.Dataset(\n",
    "    data_paths=cfg.data_paths,\n",
    "    exp_id=cfg.exp_id,\n",
    "    img_shape=cfg.img_shape,\n",
    "    img_crop_size=cfg.img_crop_size,\n",
    "    max_trace=cfg.max_trace_len,\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = data.split_training_test_data(\n",
    "    test_split=.20,\n",
    "    seed=10\n",
    ")\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n",
    "**NOTE: Remove the next cell when training your own models.** This step uses fewer ROIs (only ~3000 instead of 11 000) in the tutorial dataset to speed up computation in the tutorial. Do not do this when you are training your own data. You want to use as many data samples as possible to get the best results in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 11603\n",
      "Number of samples in test set: 2901\n"
     ]
    }
   ],
   "source": [
    "# remove or comment out this cell when using on your own data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, _, y_train, _ = train_test_split(x_train, y_train, test_size=0.75)\n",
    "\n",
    "print(f\"Number of samples in training set: {x_train.shape[0]}\") \n",
    "print(f\"Number of samples in test set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the saved models on your data\n",
    "### Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from nn.model import Model\n",
    "import optuna\n",
    "\n",
    "from nn.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test data loaded\n"
     ]
    }
   ],
   "source": [
    "data.spatial.shape, data.trace.shape, data.targets.shape\n",
    "x_train, x_test, y_train, y_test = data.split_training_test_data(\n",
    "    test_split=.20, seed=10, for_deep=True)\n",
    "\n",
    "class datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, device):\n",
    "        self.x, self.y = x, y\n",
    "        self.device = device\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        data = (self.x[0][i].to(self.device), self.x[1][i].to(self.device))\n",
    "        return data, self.y[i].to(self.device)\n",
    "\n",
    "device = 'cuda:0'\n",
    "trainsets = datasets(x_train, y_train, device)\n",
    "testsets = datasets(x_test, y_test, device)\n",
    "train_loader = torch.utils.data.DataLoader(trainsets, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(testsets, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial feature len: 2048, temporal feature len: 500\n",
      "training Results - Epoch: 1  Avg accuracy: 0.875 Avg loss: 0.328 Avg F1: 0.899\n",
      "validation Results - Epoch: 1  Avg accuracy: 0.874 Avg loss: 0.319 Avg F1: 0.898\n",
      "training Results - Epoch: 2  Avg accuracy: 0.890 Avg loss: 0.257 Avg F1: 0.913\n",
      "validation Results - Epoch: 2  Avg accuracy: 0.891 Avg loss: 0.259 Avg F1: 0.915\n",
      "training Results - Epoch: 3  Avg accuracy: 0.869 Avg loss: 0.356 Avg F1: 0.905\n",
      "validation Results - Epoch: 3  Avg accuracy: 0.872 Avg loss: 0.348 Avg F1: 0.907\n",
      "training Results - Epoch: 4  Avg accuracy: 0.891 Avg loss: 0.252 Avg F1: 0.914\n",
      "validation Results - Epoch: 4  Avg accuracy: 0.887 Avg loss: 0.256 Avg F1: 0.910\n",
      "training Results - Epoch: 5  Avg accuracy: 0.897 Avg loss: 0.244 Avg F1: 0.920\n",
      "validation Results - Epoch: 5  Avg accuracy: 0.896 Avg loss: 0.251 Avg F1: 0.919\n",
      "training Results - Epoch: 6  Avg accuracy: 0.895 Avg loss: 0.243 Avg F1: 0.918\n",
      "validation Results - Epoch: 6  Avg accuracy: 0.891 Avg loss: 0.253 Avg F1: 0.914\n",
      "training Results - Epoch: 7  Avg accuracy: 0.902 Avg loss: 0.232 Avg F1: 0.924\n",
      "validation Results - Epoch: 7  Avg accuracy: 0.901 Avg loss: 0.244 Avg F1: 0.923\n",
      "training Results - Epoch: 8  Avg accuracy: 0.902 Avg loss: 0.232 Avg F1: 0.925\n",
      "validation Results - Epoch: 8  Avg accuracy: 0.892 Avg loss: 0.250 Avg F1: 0.917\n",
      "training Results - Epoch: 9  Avg accuracy: 0.902 Avg loss: 0.236 Avg F1: 0.926\n",
      "validation Results - Epoch: 9  Avg accuracy: 0.898 Avg loss: 0.254 Avg F1: 0.923\n",
      "training Results - Epoch: 10  Avg accuracy: 0.900 Avg loss: 0.232 Avg F1: 0.926\n",
      "validation Results - Epoch: 10  Avg accuracy: 0.896 Avg loss: 0.249 Avg F1: 0.922\n",
      "training Results - Epoch: 11  Avg accuracy: 0.906 Avg loss: 0.223 Avg F1: 0.929\n",
      "validation Results - Epoch: 11  Avg accuracy: 0.897 Avg loss: 0.242 Avg F1: 0.923\n",
      "training Results - Epoch: 12  Avg accuracy: 0.906 Avg loss: 0.220 Avg F1: 0.929\n",
      "validation Results - Epoch: 12  Avg accuracy: 0.895 Avg loss: 0.242 Avg F1: 0.921\n",
      "training Results - Epoch: 13  Avg accuracy: 0.911 Avg loss: 0.210 Avg F1: 0.933\n",
      "validation Results - Epoch: 13  Avg accuracy: 0.900 Avg loss: 0.242 Avg F1: 0.925\n",
      "training Results - Epoch: 14  Avg accuracy: 0.910 Avg loss: 0.211 Avg F1: 0.932\n",
      "validation Results - Epoch: 14  Avg accuracy: 0.897 Avg loss: 0.246 Avg F1: 0.923\n",
      "training Results - Epoch: 15  Avg accuracy: 0.912 Avg loss: 0.215 Avg F1: 0.933\n",
      "validation Results - Epoch: 15  Avg accuracy: 0.895 Avg loss: 0.256 Avg F1: 0.921\n",
      "training Results - Epoch: 16  Avg accuracy: 0.918 Avg loss: 0.190 Avg F1: 0.936\n",
      "validation Results - Epoch: 16  Avg accuracy: 0.896 Avg loss: 0.247 Avg F1: 0.919\n",
      "training Results - Epoch: 17  Avg accuracy: 0.914 Avg loss: 0.210 Avg F1: 0.935\n",
      "validation Results - Epoch: 17  Avg accuracy: 0.900 Avg loss: 0.265 Avg F1: 0.925\n",
      "training Results - Epoch: 18  Avg accuracy: 0.919 Avg loss: 0.191 Avg F1: 0.937\n",
      "validation Results - Epoch: 18  Avg accuracy: 0.892 Avg loss: 0.249 Avg F1: 0.916\n",
      "training Results - Epoch: 19  Avg accuracy: 0.922 Avg loss: 0.190 Avg F1: 0.940\n",
      "validation Results - Epoch: 19  Avg accuracy: 0.894 Avg loss: 0.264 Avg F1: 0.918\n",
      "training Results - Epoch: 20  Avg accuracy: 0.919 Avg loss: 0.193 Avg F1: 0.938\n",
      "validation Results - Epoch: 20  Avg accuracy: 0.892 Avg loss: 0.262 Avg F1: 0.916\n",
      "training Results - Epoch: 21  Avg accuracy: 0.918 Avg loss: 0.193 Avg F1: 0.935\n",
      "validation Results - Epoch: 21  Avg accuracy: 0.883 Avg loss: 0.286 Avg F1: 0.907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9397879575915183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(\n",
    "    s_stage='ResNet',\n",
    "    res_block_num=5,\n",
    "    t_hidden_dim=500,\n",
    "    t_output_dim=500\n",
    ")\n",
    "train(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5 is best\n",
    "# scores = []\n",
    "# for i in range(1, 6):\n",
    "#     model = Model(\n",
    "#         s_stage='ResNet',\n",
    "#         res_block_num=i,\n",
    "#     )\n",
    "#     scores.append()\n",
    "for i in range(5):\n",
    "    print(i+1, scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimaze_san(trial):\n",
    "    block_num = trial.suggest_int('block_num', 1, 5)\n",
    "    layer_size_hop = trial.suggest_int('layer_size_hop', 2, 5)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 7, 2)\n",
    "\n",
    "    layers = [3]\n",
    "    kernels = [3]\n",
    "    for i in range(1, block_num):\n",
    "        layers.append(2 + i*layer_size_hop)\n",
    "        kernels.append(kernel_size)\n",
    "    \n",
    "    model = Model(\n",
    "        s_stage='SAN',\n",
    "        san_layers=layers,\n",
    "        san_kernels=kernels,\n",
    "    )\n",
    "    score = train(model.to(device))\n",
    "    return -score\n",
    "study = optuna.create_study()\n",
    "study.optimize(optimaze_san, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimaze_lstm(trial):\n",
    "    model = Model(\n",
    "        s_stage='ResNet',\n",
    "        res_block_num=4,\n",
    "        t_hidden_dim=trial.suggest_int('t_hidden_dim', 50, 500, 50),\n",
    "        t_output_dim=trial.suggest_int('t_output_dim', 50, 500, 50),\n",
    "    )\n",
    "    score = train(model.to(device))\n",
    "    return -score\n",
    "study = optuna.create_study()\n",
    "study.optimize(optimaze_lstm, n_trials=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# this was the final TPOT exported pipeline that acheived the highest F1 score\n",
    "tpot_model = LinearSVC(C=0.1, dual=False, loss=\"squared_hinge\", penalty=\"l1\", tol=0.1)\n",
    "tpot_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_y_pred = tpot_model.predict(x_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, tpot_y_pred))\n",
    "print(\"f1:\", f1_score(y_train, tpot_y_pred))\n",
    "dump(tpot_model, MODELDIR / f'{cfg.exp_id}_tpot.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the model finetuned on your data to use again in the future to predict without having to retrain.\n",
    "### AutoSklearn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import autosklearn\n",
    "import sklearn\n",
    "# load the AutoSklearn ensemble object\n",
    "askl = load(MODELDIR / 'cr_tutorial_askl.joblib')\n",
    "askl.refit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_automl = askl.predict(x_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, results_automl))\n",
    "print(\"f1:\", f1_score(y_test, results_automl))\n",
    "dump(askl, MODELDIR / f'{cfg.exp_id}_askl.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply classifiers to unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "askl = load(MODELDIR / f'{cfg.exp_id}_askl.joblib');\n",
    "tpot_model = load(MODELDIR / f'{cfg.exp_id}_tpot.joblib')\n",
    "cfg.img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = cr.UnlabeledDataset(\n",
    "    mat_file='../data/unlabeled_rois_DM298.mat',\n",
    "    img_shape={'x': 284, 'y': 231},\n",
    "    img_crop_size=cfg.img_crop_size,\n",
    "    max_trace=cfg.max_trace_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.img_shape, cfg.img_crop_size, cfg.max_trace_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_askl = askl.predict(unseen_data.combined)\n",
    "pred_tpot = tpot_model.predict(unseen_data.combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the ROIs labeled by askl as \"positives\"\n",
    "positive_askl = np.where(pred_askl==1)[0]\n",
    "# limit to only show 10 at once, you can play around with this of course\n",
    "cr.plot_rois(unseen_data, positive_askl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the ROIs labeled by askl as \"negatives\"\n",
    "negative_askl = np.where(pred_askl==0)[0]\n",
    "# limit to only show 10 at once, you can play around with this of course\n",
    "cr.plot_rois(unseen_data, negative_askl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label = [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "accuracy_score(gt_label, pred_askl), f1_score(gt_label, pred_askl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the ROIs labeled by TPOT as \"negatives\"\n",
    "cr.plot_rois(unseen_data, np.where(pred_tpot==0)[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data.apply_labels(pred_askl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file to check the results\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "labeled_data = loadmat('../data/unlabeled_rois_automl.mat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
